# 4. A Bayesian model with a hierarchy
To account for possible variations in skateboarders' performances between different competitions, we can build a model that uses a hierarchy. As we saw in the lectures, we can build a Bayesian hierarchy for $V_i \sim \operatorname{Ber}(\theta_i)$ if we group outcomes $v_i$ according to the different competitions. For simplicity, we use our frequentist point estimates for the parameters $\alpha_i, \beta_i$ and the parameters for $Y_i$ from task 2.
## (a) 
Assume that $\Theta_i \mid A_i=\alpha_i, B_i=\beta_i \sim \operatorname{Beta}(\alpha_i, \beta_i)$ and choose a suitable simultaneous prior distribution for $\left[\Theta_i, A_i, B_i\right]^T$. 

- Justify your choice.
1. **Likelihood**:
    - Each skateboarder's performance in a competition can be thought of as a binomial. They try a trick four times, and they succeed a certain number out of those 4 times.
    - The parameter we are interested in for this binomial distribution would be $ \Theta_i $.
    
2. **Prior for $ \Theta_i $**:
    - The success probability for each skateboarder, $ \Theta_i $, is not fixed but instead it is utfall from a Beta distribution.
    - This Beta distribution has parameters, $ A_i $ and $ B_i $.
    - The choice of the Beta distribution is strategic becouse:
        - It's constrained between 0 and 1, and we have normalaised scores between 0 and 10 meaning we want to have grom 0 to 1.
        - It's a conjugate prior for the binomial distribution, which simplifies the Bayesian update.

3. **Hyperprior for $ A_i $ and $ B_i $**:
    - The parameters $ A_i $ and $ B_i $ for the Beta distribution are not fixed either. 
    - Instead, they have to be inferred from data.
## (b) 

Generate 5000 random outcomes from the simultaneous posterior distribution
$$
f_{A_i, B_i \mid \boldsymbol{X}_i}(a_i, b_i \mid \boldsymbol{x}_i) .
$$
df_BHM

id	data
0	Majerus	[(2, 4), (1, 4)]
1	Oliveira	[(1, 4), (2, 4), (2, 4)]
2	Decenzo	[(1, 4), (2, 4), (2, 6), (2, 4)]
3	Santiago	[(0, 4), (0, 4), (1, 4)]
4	Papa	[(3, 4), (1, 4), (2, 4), (1, 4)]
5	Eaton	[(3, 4), (2, 4)]
6	Mota	[(1, 4), (1, 4), (1, 4)]
7	Shirai	[(2, 4), (2, 6), (2, 4), (1, 4), (1, 4)]
8	Jordan	[(2, 4), (1, 4), (2, 4), (2, 4), (1, 4)]
9	Hoefler	[(1, 4), (2, 4), (2, 4), (2, 6)]
10	Hoban	[(0, 4), (2, 4), (2, 4), (2, 4), (2, 6)]
11	Gustavo	[(3, 4), (0, 4), (2, 4), (2, 4), (1, 4)]
12	Ribeiro C	[(1, 4), (1, 4), (1, 4)]
13	O’neill	[(2, 4), (1, 4), (0, 4)]
14	Foy	[(3, 4), (2, 4), (1, 4)]
15	Midler	[(1, 4), (2, 4), (1, 4)]
data_population = [item for sublist in df_BHM['data'] for item in sublist]
- Bayesian hierarchical modeling is sensitive to the amount of data...
- Posterior distributions will have great uncertainty.
- The prior distribution will have greater influence on the posterior when the likelihood thats based on data is weak becouse of few data points.
- Overfitting.
***Or I could use the same prior from previous problem***
def log_prior_BH(alpha, beta):
    
    precision = 5
    lambda_hyper = 0.5
    
    theta_hyper = lambda_hyper*precision
    return   theta_hyper * np.log(lambda_hyper) - loggamma(theta_hyper) + (theta_hyper - 1) * np.log(alpha + beta + 1) - lambda_hyper * (alpha + beta + 1) - np.log(alpha + beta)
    def log_posterior_BH(alpha, beta, data):
    log_p = log_prior_BH(alpha, beta)

    for x, n in data:
        log_p += loggamma(alpha + beta) - (loggamma(alpha) + loggamma(beta))
        log_p += loggamma(alpha + x) + loggamma(beta + n - x) - loggamma(alpha + beta + n)
        
    return log_p
 # Parameters
delta = 0.1
k = 50000 # How many we run
num_samples = 50000 # How many we keep

# Initialize an empty dictionary to store the posterior samples for each skateboarder
skateboarder_samples = {}

# Loop through each row in the df_BHM DataFrame
for index, row in df_BHM.iterrows():
    skateboarder_name = row['id']
    skateboarder_data = row['data']
    
    alphas = np.zeros(num_samples)
    betas = np.zeros(num_samples)

    alphas[0] = np.exp(stats.cauchy.rvs(size=1)[0])
    betas[0] = np.exp(stats.cauchy.rvs(size=1)[0])
    delta = 0.1

    for _ in range(num_samples - 1):
        
        last_alpha = alphas[_]
        last_beta = betas[_]
        
        proposal_alpha = np.exp(np.log(last_alpha) + stats.norm.rvs(0.5, size=1)[0])
        proposal_beta = np.exp(np.log(last_beta) + stats.norm.rvs(0.5, size=1)[0])
        log_rho = log_posterior_BH(proposal_alpha, proposal_beta, skateboarder_data) - log_posterior_BH(last_alpha, last_beta, skateboarder_data)
        
        u = stats.uniform.rvs()
        
        if np.log(u) <= log_rho:
            alphas[_ + 1] = proposal_alpha
            betas[_ + 1] = proposal_beta
        else:
            alphas[_ + 1] = last_alpha
            betas[_ + 1] = last_beta

    # Storing the results as a tuple (alpha array, beta array)
    skateboarder_samples[skateboarder_name] = (alphas, betas)
   sns.set_style("whitegrid")

# Color palette
colors = sns.color_palette("deep")

# Create a subplot for each skateboarder
fig, axes = plt.subplots(nrows=len(df_BHM), ncols=3, figsize=(15, 4 * len(df_BHM)))

# Lists to store cumulative means and variances of alphas and betas
alpha_cumulative_means = []
beta_cumulative_means = []
alpha_variances = []
beta_variances = []

# Iterate over each skateboarder
for idx, (skateboarder, samples) in enumerate(skateboarder_samples.items()):
    alpha_values, beta_values = samples  # Unpack the tuple directly

    # Compute cumulative averages of the last 5000 alpha values and beta values
    alpha_cumulative_mean = np.mean(alpha_values[-5000:])
    beta_cumulative_mean = np.mean(beta_values[-5000:])
    
    # Compute variances for the last 5000 alpha and beta values
    alpha_variance = np.var(alpha_values[-5000:])
    beta_variance = np.var(beta_values[-5000:])
    
    # Store the cumulative averages and variances
    alpha_cumulative_means.append(alpha_cumulative_mean)
    beta_cumulative_means.append(beta_cumulative_mean)
    alpha_variances.append(alpha_variance)
    beta_variances.append(beta_variance)

    # Plot cumulative average of alphas
    axes[idx, 0].plot(np.cumsum(alpha_values) / np.arange(1, len(alpha_values) + 1), color=colors[0], linewidth=1.5)
    axes[idx, 0].set_title(f"{skateboarder} - Alphas")

    # Plot cumulative average of betas
    axes[idx, 1].plot(np.cumsum(beta_values) / np.arange(1, len(beta_values) + 1), color=colors[1], linewidth=1.5)
    axes[idx, 1].set_title(f"{skateboarder} - Betas")

    # Scatter plot for alphas and betas
    axes[idx, 2].scatter(alpha_values, beta_values, color=colors[2], s=10, alpha=0.5)
    axes[idx, 2].set_title(f"{skateboarder} - Alphas vs Betas")
    axes[idx, 2].set_xlabel('Alphas')
    axes[idx, 2].set_ylabel('Betas')

plt.tight_layout()
plt.show()

# Add the cumulative averages and variances to the dataframe
df_BHM['alpha_mean'] = alpha_cumulative_means
df_BHM['beta_mean'] = beta_cumulative_means
df_BHM['alpha_variance'] = alpha_variances
df_BHM['beta_variance'] = beta_variances
 df_BHM[["id","alpha_mean", "beta_mean"]]
 	id	alpha_mean	beta_mean
0	Majerus	2.226499	3.036001
1	Oliveira	2.361866	3.040581
2	Decenzo	2.408359	3.392507
3	Santiago	0.776480	3.676554
4	Papa	2.601304	3.083483
5	Eaton	3.085916	2.244104
6	Mota	1.940925	4.096838
7	Shirai	2.634055	3.873805
8	Jordan	2.702725	3.724393
9	Hoefler	2.450997	3.504148
10	Hoban	1.995066	3.341174
11	Gustavo	2.213723	3.182604
12	Ribeiro C	1.873619	3.766154
13	O’neill	1.354807	3.193182
14	Foy	2.614442	2.560672
15	Midler	2.166905	3.408462
   Use your simulations to generate 5000 random outcomes from the marginal posterior distribution $\Theta_i \mid \boldsymbol{X}_i=\boldsymbol{x}_i$.
 # Here i will get 5000 samples for my theta
num_outcomes = 5000

theta_outcomes = {}

# Iterate over each row in df_BHM
for _, row in df_BHM.iterrows():
    skateboarder_name = row['id']
    alpha_avg = row['alpha_mean']  
    beta_avg = row['beta_mean']
    
    # Generate 5000 theta samples using the alpha and beta values for the skateboarder
    theta_samples = stats.beta.rvs(alpha_avg, beta_avg, size=num_outcomes)
    theta_outcomes[skateboarder_name] = theta_samples

# Initialize a list to store the theta samples for each skateboarder in df_BHM
theta_list = []

# Iterate over each row in df_BHM to fetch and store the corresponding theta samples
for _, row in df_BHM.iterrows():
    skateboarder_name = row['id']
    theta_samples = theta_outcomes[skateboarder_name]
    theta_list.append(theta_samples)

# Add the theta samples to df_BHM as a new column
df_BHM['theta'] = theta_list
  Plot your outcomes for the following posterior distributions:
$$
f_{\theta_i \mid \boldsymbol{X}_i}(\theta_i \mid \boldsymbol{x}_i) \quad \text{and} \quad f_{A_i, B_i \mid \boldsymbol{X}_i}(a_i, b_i \mid \boldsymbol{x}_i) ,
$$
# Setup style
sns.set_style("whitegrid")

# Set up a 4x4 figure layout
fig, axes = plt.subplots(nrows=4, ncols=4, figsize=(14, 14))

# Flatten the axes to iterate easily
axes_flat = axes.flatten()

# Iterate over each skateboarder in the dataframe
for idx, row in df_BHM.iterrows():
    skateboarder_name = row['id']

    # Histogram for theta samples
    axes_flat[idx].hist(row['theta'], bins=30, density=True, alpha=0.7, color='skyblue', edgecolor='black')
    axes_flat[idx].set_title(skateboarder_name, fontsize=14)
    axes_flat[idx].set_xlabel('Theta Value', fontsize=12)
    axes_flat[idx].set_ylabel('Density', fontsize=12)
    axes_flat[idx].grid(axis='y', linestyle='--', alpha=0.7)

# Hide any remaining empty subplots
for i in range(len(df_BHM), len(axes_flat)):
    axes_flat[i].axis('off')

# Set a main title for the figure
fig.suptitle('Distribution of Theta Values', fontsize=16)

# Adjust layout
plt.tight_layout(rect=[0, 0.03, 1, 0.95])
plt.show()
$$
\begin{aligned}
& E\left(\theta_i\right)=\frac{1}{N} \sum_{j=1}^N \theta_{i, j} \\
& \operatorname{Var}\left(\theta_i\right)=\frac{1}{N-1} \sum_{j=1}^N\left(\theta_{i, j}-E\left(\theta_i\right)\right)^2
\end{aligned}
$$
# Computing posterior expected value and variance for theta
df_BHM['theta_mean'] = df_BHM['theta'].apply(np.mean)
df_BHM['theta_variance'] = df_BHM['theta'].apply(np.var)

# Display the dataframe
df_BHM_sorted = df_BHM.sort_values(by='id')[['id', 'theta_mean', 'theta_variance', 'alpha_mean', 'alpha_variance', 'beta_mean', 'beta_variance']]
display(df_BHM_sorted)
	id	theta_mean	theta_variance	alpha_mean	alpha_variance	beta_mean	beta_variance
2	Decenzo	0.418466	0.036094	2.408359	2.147318	3.392507	3.716851
5	Eaton	0.585080	0.038249	3.085916	6.740041	2.244104	3.263786
14	Foy	0.502731	0.041244	2.614442	3.009007	2.560672	3.036840
11	Gustavo	0.410729	0.038492	2.213723	1.717870	3.182604	3.679734
10	Hoban	0.369759	0.036299	1.995066	1.521243	3.341174	4.244371
9	Hoefler	0.409567	0.034347	2.450997	2.003306	3.504148	4.067933
8	Jordan	0.418873	0.033193	2.702725	2.523138	3.724393	3.807570
0	Majerus	0.420976	0.038352	2.226499	2.577471	3.036001	4.333149
15	Midler	0.388224	0.036098	2.166905	2.210025	3.408462	4.724580
6	Mota	0.325182	0.031672	1.940925	1.874318	4.096838	7.517977
1	Oliveira	0.439022	0.037472	2.361866	2.360888	3.040581	3.508945
13	O’neill	0.302528	0.038074	1.354807	0.917724	3.193182	4.738033
4	Papa	0.461618	0.038927	2.601304	2.484797	3.083483	3.398322
12	Ribeiro C	0.334478	0.033452	1.873619	2.066278	3.766154	6.021909
3	Santiago	0.174507	0.026451	0.776480	0.449377	3.676554	7.092605
7	Shirai	0.399772	0.031472	2.634055	2.553833	3.873805	4.825812
- How do these variances for $\theta_i$ compare to the variances for $\theta_i$ computed for the model in Task 3?
initial_bayesian_df_sorted = initial_bayesian_df_tricks.sort_values(by='id')[['id', "theta_bayes_mean", 'theta_bayes_s^2']]
display(initial_bayesian_df_sorted)

id	theta_bayes_mean	theta_bayes_s^2
0	Decenzo	0.444394	0.013706
1	Eaton	0.598614	0.022560
2	Foy	0.500434	0.017024
3	Gustavo	0.406197	0.010801
4	Hoban	0.406687	0.010776
5	Hoefler	0.445578	0.012819
6	Jordan	0.411506	0.010217
7	Majerus	0.401588	0.021915
8	Midler	0.355392	0.015372
9	Mota	0.285128	0.013450
10	Oliveira	0.428240	0.016259
11	O’neill	0.286351	0.014073
12	Papa	0.446550	0.013482
13	Ribeiro C	0.285590	0.014097
14	Santiago	0.144008	0.008488
15	Shirai	0.407948	0.010619
Comparing the values between the two:

**Theta which we can interpret to signify the Performance Ability to land a trick:**
- The values of `theta_mean` int the Bayesian and the Bayesian Hierarciacl model are relatively close. 
- This would mean that the estimation ability between the two models is quite close as well.

- The values of `theta_variance` differ quite alot. The values of variance for Bayesian model are smaller compared to the Hierarchical Bayesian model. A smaller variance means that there is higher confidence in the performance ability of the model.
- The high variance in the Hierarchical model's estimates could be caused by the nature of the model, where they incorporate variations at different levels 
- Individual skateboarders.

***The key takeaway is understanding the trade-off between the two models:***
- ***The Hierarchical model provides a more comprehensive view, which might come with more uncertainty in individual predictions.***
- ***The Bayesian model might be more certain but potentially overfitting to individual data.***
## (c) 
Using your $5000$ samples from part (b), simulate 5000 LCQ competition winners and calculate the mode of the results. 

- What are the respective estimated probabilities for the actual winners and your mode value?
### Prepare the data:
data_for_simulations = df_BHM.copy()

#From Bayesian Model
data_for_simulations["alpha_trick_Bayes"] = initial_bayesian_df_tricks["alpha_bayes_X_mean"]
data_for_simulations["beta_trick_Bayes"] = initial_bayesian_df_tricks["beta_bayes_X_mean"]

data_for_simulations["alpha_run_Bayes"] = initial_bayesian_df_runs["alpha_bayes_Y_mean"]
data_for_simulations["beta_run_Bayes"] = initial_bayesian_df_runs["beta_bayes_Y_mean"]

#From Frequentist Model
data_for_simulations["alpha_trick_Frequentist"] = results_df_Moment_Estimators["alpha_trick"]
data_for_simulations["beta_trick_Frequentist"] = results_df_Moment_Estimators["beta_trick"]

data_for_simulations["alpha_run_Frequentist"] = results_df_Moment_Estimators["alpha_run"]
data_for_simulations["beta_run_Frequentist"] = results_df_Moment_Estimators["beta_run"]


#"theta" is the same for both models
data_for_simulations [["id", "alpha_trick_Bayes", "beta_trick_Bayes", "alpha_run_Bayes", "beta_run_Bayes","alpha_trick_Frequentist", "beta_trick_Frequentist", "alpha_run_Frequentist", "beta_run_Frequentist"]]
import pandas as pd
import numpy as np
from scipy import stats

# Checking if the trick was successful
def successful_trick(theta):
    return bool(stats.bernoulli.rvs(theta))

# Score from beta distribution for a trick
def trick_score(alpha, beta):
    return stats.beta.rvs(alpha, beta, size=1)[0]

# Score from beta distribution for a run
def run_score(alpha, beta):
    return stats.beta.rvs(alpha, beta, size=1)[0]

# Compute the total score: max run + top 2 trick scores
def total_score(trick_scores, run_scores):
    return max(run_scores) + sum(sorted(trick_scores)[-2:])

# Sort the DataFrame based on total score
def sort_my_df(df):
    return df.sort_values(by='total score', ascending=False)

# Simulate a LCQ
def simulate_lcq(data_for_simulations, theta_index):
    new_lcq_df = pd.DataFrame(columns=["id", "run 1", "run 2", "trick 1", "trick 2", "trick 3", "trick 4", "total score"])
    
    for index, row in data_for_simulations.iterrows():
        theta = row['theta'][theta_index]  # use specific theta for this simulation
        alpha_trick = row['alpha_trick_Bayes']
        beta_trick = row['beta_trick_Bayes']
        alpha_run = row['alpha_run_Bayes']
        beta_run = row['beta_run_Bayes']

        new_lcq_df.at[index, "id"] = row['id']

        # Simulate tricks
        for i in range(4):
            if successful_trick(theta):
                trick = trick_score(alpha_trick, beta_trick)
            else:
                trick = 0
            new_lcq_df.at[index, f"trick {i+1}"] = round(trick,1)

        # Simulate runs
        for i in range(2):
            run = run_score(alpha_run, beta_run)
            new_lcq_df.at[index, f"run {i+1}"] = round(run,1)

        # Calculate total score
        total = total_score(new_lcq_df.loc[index, ['trick 1', 'trick 2', 'trick 3', 'trick 4']], new_lcq_df.loc[index, ['run 1', 'run 2']])
        new_lcq_df.at[index, "total score"] = round(total,1)

    return new_lcq_df

# Extract the top four skateboarders
def find_top_four_skateboarders(df):
    return sort_my_df(df).head(4)

# Initialize the points counter
point_counts = {name: 0 for name in data_for_simulations['id']}

# Main simulation logic
top_four_rankings = []

for theta_index in range(5000):
    lcq_results = simulate_lcq(data_for_simulations, theta_index)
    top_four_df = find_top_four_skateboarders(lcq_results)

    # Extract IDs of the top four skateboarders and update their points
    top_four_ids = top_four_df['id'].tolist()[:4]
    top_four_rankings.append(top_four_ids)
    for skater_id in top_four_ids:
        point_counts[skater_id] += 1

# Rank skateboarders based on their points
df_ranked = pd.DataFrame(list(point_counts.items()), columns=['id', 'points'])
df_ranked = df_ranked.sort_values(by='points', ascending=False)
df_ranked['rank'] = df_ranked['points'].rank(ascending=False)

# Find mode (or any other operations you want to perform)
find_mode_and_occurences_of_specific(top_four_rankings, ["Gustavo", "Decenzo", "Eaton", "Hoban"])
print("\nRanked Data Based on Simulations:")
df_ranked