Bayesian Model
As an alternative to the frequentist model developed in Task 2, we can consider a Bayesian model.
$$
\Theta_i \perp A_i, B_i
$$
$$
f_{\Theta_i, A_i, B_i}\left(\theta_i, \alpha_i, \beta_i\right)=f_{\Theta_i}\left(\theta_i\right) f_{A_i, B_i}\left(\alpha_i, \beta_i\right)
$$
## (a) Propose a joint prior distribution for the parameters $[\Theta_i, A_i, B_i]^T$ for $X_i$ where we assume $\Theta_i \perp A_i, B_i$ for all $i$. 
- $\Theta_i$ represents the average value of a skateboarder landing a trick. 
- Since $\Theta_i$ is an average value, it will lie between 0 and 1. 
- A natural choice for a prior distribution for a parameter that lies between 0 and 1 is the Beta distribution since it is defined on the interval [0,1]. 
### Prior for $\Theta_i$:
If we use a non-informative prior, no strong beliefs about the distribution of $\Theta_i$ before observing the data. 
A common choice, according to the notes, would be $Beta(1,1)$, which is equivalent to a uniform distribution on [0,1].

$$
f_{\Theta_i}\left(\theta_i\right)=\operatorname{Beta}\left(\theta_i ; 1,1\right)=U(0,1)
$$
### Prior for  $A_i$, $B_i$:
- Since $A_i$, $B_i$ are are parameters for a distribution that produces results between 0 and 1, a natural choice for their prior distribution is the Beta distribution.
## (b) Generate 5000 random samples from the posterior distribution
$$
f_{\Theta_i, \alpha_i, \beta_i | \boldsymbol{X}_i}(\theta_i, \alpha_i, \beta_i | \boldsymbol{x}_i) .
$$
Plot your resulting samples for the marginal posterior distributions:
$$
f_{\theta_i | \boldsymbol{X}_i}(\theta_i | \boldsymbol{x}_i) \quad \text{and} \quad f_{\alpha_i, \beta_i | \boldsymbol{X}_i}(\alpha_i, \beta_i | \boldsymbol{x}_i) .
$$

- Calculate the posterior sample mean and the posterior sample variance for each parameter $ \theta_i, \alpha_i $, and $ \beta_i $ for all skateboarders.
$$
X_i \mid \Theta_i=\theta_i, A_i=\alpha_i, B_i=\beta_i
$$


- $ \Theta_i$ represents the average value of a skateboarder landing a trick.
- $ A_i$ and $ B_i$ are parameters for a distribution.

We are interested in the behavior or distribution of $ X_i$, the observed data for skateboarder $ i$, when the parameters $\Theta_i$, $ A_i$, and $ B_i$ are fixed at specific values.
Given the above, the joint prior distribution for $\theta_i, A_i$, and $B_i$ is:
$$
f_{\Theta_i, A_i, B_i}\left(\theta_i, \alpha_i, \beta_i\right)=f_{\Theta_i}\left(\theta_i\right)  f_{A_i, B_i}\left(\alpha_i, \beta_i\right)
$$

Given the likelihood $f_{\mathbb{X} \mid \Theta_i A_i, B_i}\left(x \mid \theta_i, \alpha_i, \beta_i\right)$, the posterior distribution is:
$$
f_{\Theta_i, A_i, B_i \mid \boldsymbol{X}_i}\left(\theta_i, \alpha_i, \beta_i \mid \boldsymbol{x}_i\right) \propto f_{\Theta_i, A_i, B_i}\left(\theta_i, \alpha_i, \beta_i\right) f_{\mathbb{X} \mid \Theta_i A_i, B_i}\left(x \mid \theta_i, \alpha_i, \beta_i\right)
$$
$$
f_{\Theta_i, A_i, B_i \mid \boldsymbol{X}_i}\left(\theta_i, \alpha_i, \beta_i \mid \boldsymbol{x}_i\right) \propto f_{\Theta_i}\left(\theta_i\right)  f_{A_i, B_i}\left(\alpha_i, \beta_i\right) f_{\mathbb{X} \mid \Theta_i A_i, B_i}\left(x \mid \theta_i, \alpha_i, \beta_i\right)
$$
We can use Metropolis Algorithm on this part: 

$$
f_{\Theta_i, A_i, B_i}\left(\theta_i, \alpha_i, \beta_i\right) f_{\mathbb{X} \mid \Theta_i A_i, B_i}\left(\mathbb{x} \mid \theta_i, \alpha_i, \beta_i\right)
$$
##### Apriori $f_{\Theta_i}(\theta_i)$
- Assuming that the trick was landed, we can simply use $f_{\Theta_i}(\theta_i) = 1$.
- This will greatly simplify our equation!
##### Apriori $f_{A, B}(\alpha, \beta)$
We can use reparametrization for $f_{A, B}(\alpha, \beta)$:
$$
\mu=\frac{\alpha}{\alpha+\beta} \quad \text { och } \quad \kappa=\alpha+\beta+1
$$
To see that $\kappa$ is a measure of precision, note that for $X \sim \operatorname{Beta}(\alpha, \beta)$,
$$
\operatorname{Var}[X]=\frac{\alpha \beta}{(\alpha+\beta)^2(\alpha+\beta+1)}=\frac{\mu(1-\mu)}{\kappa} ;
$$
i.e., $\kappa$ is inversely proportional to $\operatorname{Var}[X]$.
- We can then specify a prior distribution for $[\mu, \kappa]^{\mathrm{T}}$ and take a transformation of stochastic variables to obtain a prior distribution for $[\alpha, \beta]^{\mathrm{T}}$. 
- We form a conditional prior distribution for $[\mu, \kappa]^{\mathrm{T}}$ according to the factorization
$$
(\mu, \kappa)=(\kappa \mid \mu) f(\mu)
$$
where we take
$$
\begin{aligned}
\kappa \mid \mu & \sim \operatorname{Gamma}(\theta, \lambda), \\
\mu & \sim \mathrm{U}(0,1) .
\end{aligned}
$$
This gives us the distribution
$$
f_{A, B}(\alpha, \beta)=\frac{\lambda^\theta}{\Gamma(\theta)}(\alpha+\beta+1)^{\theta-1} e^{-\lambda(\alpha+\beta+1)}(\alpha+\beta)^{-1}
$$
##### Data distirbution $f_{\mathbb{X} \mid \Theta_i A_i, B_i}\left(x \mid \theta_i, \alpha_i, \beta_i\right)$
We know from the problem statement that:
$$
f_{X_i}\left(x_i \mid \theta_i, \alpha_i, \beta_i\right)=\left(1-\theta_i\right) \mathbf{1}_{x_i=0}+\theta_i f_{Z_i}\left(z_i\right)
$$
We substitute in the $Beta(\alpha_i, \beta_i)$:
$$
f_{X_i}\left(x_i \mid \theta_i, \alpha_i, \beta_i\right)=\left(1-\theta_i\right) \mathbf{1}_{x_i=0}+\theta_i \frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)+\Gamma(\beta)} z_i^{\alpha-1}(1-z)^{\beta-1}\mathbf{1}_{x_i \neq 0}
$$
##### Thus, if we combine everything $f_{\Theta_i, A_i, B_i \mid \boldsymbol{X}_i}\left(\theta_i, \alpha_i, \beta_i \mid \boldsymbol{x}_i\right) \propto f_{\Theta_i}\left(\theta_i\right)  f_{A_i, B_i}\left(\alpha_i, \beta_i\right) f_{\mathbb{X} \mid \Theta_i A_i, B_i}\left(x \mid \theta_i, \alpha_i, \beta_i\right)$:
$$
1\times \left(\frac{\lambda^{\theta_{hyper}}_{hyper}}{\Gamma(\theta_{hyper})}(\alpha+\beta+1)^{\theta_{hyper}-1} e^{-\lambda_{hyper}(\alpha+\beta+1)}(\alpha+\beta)^{-1}\right)\times\left( \left(1-\theta_i\right) \mathbf{1}_{x_i=0}+\theta_i \frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)+\Gamma(\beta)} z_i^{\alpha-1}(1-z)^{\beta-1}\mathbf{1}_{x_i \neq 0}\right)
$$

- This appears quite complex to implement, so let's log everything.
$\log f_{A_j B_j }\left(\alpha_j, \beta_j \right)=$
$$
\theta_{hyper} \log (\lambda_{hyper})-\log (\Gamma(\theta_{hyper}))+(\theta_{hyper}-1) \log (\alpha+\beta+1)-\lambda_{hyper}(\alpha+\beta+1)-\log (\alpha+\beta)
$$
$\log f_{\mathbb{X} \mid \Theta_i A_i, B_i}\left(x \mid \theta_i, \alpha_i, \beta_i\right)=$
$$
\log(1-\theta) \cdot \mathbb{1}_{\{x_i=0\}} + \left(\log(\theta_i) + \log(\Gamma(\alpha+\beta)) - \log(\Gamma(\alpha)) - \log(\Gamma(\beta)) + (\alpha-1) \log(z_i) + (\beta-1) \log(1-z_i)\right)\mathbf{1}_{x_i \neq 0}.
$$
def log_prior(alpha, beta, precision, lambda_hyper):
    theta_hyper = lambda_hyper*precision
    return   theta_hyper * np.log(lambda_hyper) - loggamma(theta_hyper) + (theta_hyper - 1) * np.log(alpha + beta + 1) - lambda_hyper * (alpha + beta + 1) - np.log(alpha + beta)
    
***This is my log_posterior.***
$\log f_{\mathbb{X} \mid \Theta_i A_i, B_i}\left(x \mid \theta_i, \alpha_i, \beta_i\right)=$
$$
\log(1-\theta) \cdot \mathbb{1}_{\{x_i=0\}} + \left(\log(\theta_i) + \log(\Gamma(\alpha+\beta)) - \log(\Gamma(\alpha)) - \log(\Gamma(\beta)) + (\alpha-1) \log(z_i) + (\beta-1) \log(1-z_i)\right)\mathbf{1}_{x_i \neq 0}.
$$
def log_posterior(alpha, beta, theta, data):
    precision = 5
    lambda_hyper = 0.5

    log_p = log_prior(alpha, beta, precision, lambda_hyper)

    for z_i in data:
        
        if z_i == 0:
            log_p += np.log(1 - theta)
        else:
            log_p += np.log(theta) + loggamma(alpha + beta) - loggamma(alpha) - loggamma(beta) + (alpha - 1) * np.log(z_i) + (beta-1) * np.log(1 - z_i)

    return log_p
    def metropolis_algorithm(data, initial_guess, number_of_samples):
    
    alphas = np.zeros((number_of_samples))
    betas = np.zeros((number_of_samples))
    thetas = np.ones((number_of_samples))

    thetas [0] = initial_guess[0]
    alphas[0] = initial_guess[1] 
    betas[0] = initial_guess[2]
    
    
    for i in range(number_of_samples - 1):
        last_alpha = alphas[i]
        last_beta = betas[i]
        last_theta = thetas[i]
        
       # Att exponentiera ser till att alpha och beta är positivt, hade abs innan men det kan leda till alpha och beta med bias och det gick snabbare att konvergera
        proposal_alpha =  np.exp(np.log(last_alpha) + stats.norm.rvs( 0.5, size=1))[0]
        proposal_beta =   np.exp(np.log(last_beta) + stats.norm.rvs(0.5, size=1))[0]
        
        proposal_theta = abs(stats.uniform.rvs(0,1, size = 1))[0]

        
        # Note that the acceptance probability rho is calculated for the *pair* of
        # proposed samples.
        log_rho = log_posterior(proposal_alpha, proposal_beta, proposal_theta, data) - log_posterior(last_alpha, last_beta, last_theta, data)
        
        u = stats.uniform.rvs()
        
        if np.log(u) <= log_rho:
            alphas[i + 1] = proposal_alpha
            betas[i + 1] = proposal_beta
            thetas[i+1] = proposal_theta
        else:
            alphas[i + 1] = last_alpha
            betas[i + 1] = last_beta
            thetas[i +1] = last_theta
    
    return alphas, betas, thetas
    import pandas as pd
import numpy as np
from scipy import stats

# Your metropolis_algorithm function...

# Convert your DataFrame to a list of dictionaries
data_list = initial_bayesian_df_tricks.to_dict(orient='records')

# Create an empty list to store the results
results = []

for skateboarder in data_list:
    tricks = skateboarder['tricks']
    
    # Extracting initial guesses
    initial_guesses = [
        [skateboarder['theta_average'], skateboarder['alpha_trick_Moment'], skateboarder['beta_trick_Moment']],
        [skateboarder['theta_average'], skateboarder['alpha_trick_Gradient'], skateboarder['beta_trick_Gradient']],
        [skateboarder['theta_average'], skateboarder['alpha_trick_Gauss'], skateboarder['beta_trick_Gauss']],
        [skateboarder['theta_average'], skateboarder['population_alpha'], skateboarder['population_beta']],
        [skateboarder['theta_average'], skateboarder['population_alpha0'], skateboarder['population_beta0']]
    ]
    
    number_of_samples=15000
    # Run the Metropolis algorithm for each initial guess
    moment_chain = metropolis_algorithm(tricks, initial_guesses[0], number_of_samples)
    gradient_chain = metropolis_algorithm(tricks, initial_guesses[1], number_of_samples)
    gauss_chain = metropolis_algorithm(tricks, initial_guesses[2], number_of_samples)
    population_chain = metropolis_algorithm(tricks, initial_guesses[3], number_of_samples)
    population0_chain = metropolis_algorithm(tricks, initial_guesses[4], number_of_samples)
    
    # Store results in the specified format
    skateboarder['moment_chain'] = moment_chain
    skateboarder['gradient_chain'] = gradient_chain
    skateboarder['gauss_chain'] = gauss_chain
    skateboarder['population_chain'] = population_chain
    skateboarder['population0_chain'] = population0_chain
    
    results.append(skateboarder)

# Convert results back to DataFrame
initial_bayesian_df_tricks = pd.DataFrame(results)
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns  # Seaborn is great for styling and color palettes

# Use a Seaborn palette for more advanced colors
colors = sns.color_palette("deep", 5)

# Adjust column count to 4 to add space for scatter plots
fig, axes = plt.subplots(nrows=len(initial_bayesian_df_tricks), ncols=4, figsize=(20, 4 * len(initial_bayesian_df_tricks)))

for idx, skateboarder in initial_bayesian_df_tricks.iterrows():
    # Extract chains
    chains = [
        skateboarder['moment_chain'],
        skateboarder['gradient_chain'],
        skateboarder['gauss_chain'],
        skateboarder['population_chain'],
        skateboarder['population0_chain']
    ]
    
    # Extract and plot values
    for chain, color in zip(chains, colors):
        alpha_values, beta_values, theta_values = chain
        
        # Plot cumulative average of alphas
        axes[idx, 0].plot(np.cumsum(alpha_values) / np.arange(1, len(alpha_values) + 1), color=color, linewidth=1.5)
        axes[idx, 0].set_title(f"{skateboarder['id']} - Alphas")
        axes[idx, 0].grid(True, which="both", linestyle='--', linewidth=0.5)
        
        # Plot cumulative average of betas
        axes[idx, 1].plot(np.cumsum(beta_values) / np.arange(1, len(beta_values) + 1), color=color, linewidth=1.5)
        axes[idx, 1].set_title(f"{skateboarder['id']} - Betas")
        axes[idx, 1].grid(True, which="both", linestyle='--', linewidth=0.5)
        
        # Plot cumulative average of thetas
        axes[idx, 2].plot(np.cumsum(theta_values) / np.arange(1, len(theta_values) + 1), color=color, linewidth=1.5)
        axes[idx, 2].set_title(f"{skateboarder['id']} - Thetas")
        axes[idx, 2].grid(True, which="both", linestyle='--', linewidth=0.5)
        
        # Scatter plot for alphas and betas in the fourth column
        axes[idx, 3].scatter(alpha_values, beta_values, color=color, s=10, alpha=0.5)
        axes[idx, 3].set_title(f"{skateboarder['id']} - Alphas vs Betas")
        axes[idx, 3].set_xlabel('Alphas')
        axes[idx, 3].set_ylabel('Betas')
        axes[idx, 3].grid(True, which="both", linestyle='--', linewidth=0.5)

# Adjust aesthetics for all plots
for ax_row in axes:
    for ax in ax_row:
        sns.despine(ax=ax)  # Remove top and right spines for a cleaner look
        ax.tick_params(axis="both", which="both", length=0)  # Remove tick marks

# Tight layout
plt.tight_layout()
plt.show()
initial_bayesian_df_tricks['alpha_bayes_X_mean'] = np.nan
initial_bayesian_df_tricks['beta_bayes_X_mean'] = np.nan
initial_bayesian_df_tricks['theta_bayes_mean'] = np.nan
initial_bayesian_df_tricks['alpha_bayes_X_s^2'] = np.nan
initial_bayesian_df_tricks['beta_bayes_X_s^2'] = np.nan
initial_bayesian_df_tricks['theta_bayes_s^2'] = np.nan

chains = ['moment_chain', 'gradient_chain', 'gauss_chain', 'population_chain', 'population0_chain']

for idx, skateboarder in initial_bayesian_df_tricks.iterrows():
    
    # Combine from all chains
    all_alphas = np.concatenate([skateboarder[chain][0] for chain in chains])
    all_betas = np.concatenate([skateboarder[chain][1] for chain in chains])
    all_thetas = np.concatenate([skateboarder[chain][2] for chain in chains])

    # Update the dataframe
    initial_bayesian_df_tricks.at[idx, 'alpha_bayes_X_mean'] = np.mean(all_alphas)
    initial_bayesian_df_tricks.at[idx, 'beta_bayes_X_mean'] = np.mean(all_betas)
    initial_bayesian_df_tricks.at[idx, 'theta_bayes_mean'] = np.mean(all_thetas)
    
    initial_bayesian_df_tricks.at[idx, 'alpha_bayes_X_s^2'] = np.var(all_alphas, ddof=1)
    initial_bayesian_df_tricks.at[idx, 'beta_bayes_X_s^2'] = np.var(all_betas, ddof=1)
    initial_bayesian_df_tricks.at[idx, 'theta_bayes_s^2'] = np.var(all_thetas, ddof=1)

initial_bayesian_df_tricks[['id', "alpha_bayes_X_mean", "beta_bayes_X_mean", "theta_bayes_mean", "alpha_bayes_X_s^2", "beta_bayes_X_s^2", "theta_bayes_s^2"]]
# Function to compute cumulative average
def cumulative_avg(data):
    return np.cumsum(data) / np.arange(1, len(data) + 1)

# Adding new columns to the dataframe before filling them
chains_list = ['moment', 'gradient', 'gauss', 'population', 'population0']
for chain_name in chains_list:
    for param in ['alpha', 'beta', 'theta']:
        initial_bayesian_df_tricks[f'{chain_name}_{param}_cumavg'] = pd.Series(dtype=object)

# Iterate through each row of the DataFrame
for idx, skateboarder in initial_bayesian_df_tricks.iterrows():
    # Extract chains
    chains = [
        skateboarder['moment_chain'],
        skateboarder['gradient_chain'],
        skateboarder['gauss_chain'],
        skateboarder['population_chain'],
        skateboarder['population0_chain']
    ]
    
    # For each chain, compute cumulative averages and save to new columns
    for chain_name, chain in zip(chains_list, chains):
        alpha_values, beta_values, theta_values = chain
        initial_bayesian_df_tricks.at[idx, f'{chain_name}_alpha_cumavg'] = cumulative_avg(alpha_values)
        initial_bayesian_df_tricks.at[idx, f'{chain_name}_beta_cumavg'] = cumulative_avg(beta_values)
        initial_bayesian_df_tricks.at[idx, f'{chain_name}_theta_cumavg'] = cumulative_avg(theta_values)
        ## (c) 
- Propose a (joint) prior distribution for the parameters of your model $Y_i$ from task $2(c)$ and justify your choice. 

- You can assume that the model's parameters for skateboarder $i$ are independent of all other parameters including $\theta_i$, $\alpha_i$, and $\beta_i$. 

- Generate 5000 samples from the posterior distribution (make sure to save these samples!) and create a scatter plot of the results. 

- What is the sample mean and sample variance for each of your parameters based on your outcomes?
results_df_Moment_Estimators, results_df_Gradient_Descent, results_df_Newton_Raphson

#Get Data
initial_bayesian_df_runs = all_runs_df.copy()

initial_bayesian_df_runs['alpha_run_Moment'] = results_df_Moment_Estimators['alpha_run']
initial_bayesian_df_runs['beta_run_Moment'] = results_df_Moment_Estimators['beta_run']

initial_bayesian_df_runs['alpha_run_Gradient'] = results_df_Gradient_Descent['alpha_run']
initial_bayesian_df_runs['beta_run_Gradient'] = results_df_Gradient_Descent['beta_run']

initial_bayesian_df_runs['alpha_run_Gauss'] = results_df_Newton_Raphson['alpha_run']
initial_bayesian_df_runs['beta_run_Gauss'] = results_df_Newton_Raphson['beta_run']
##### Initial guesses via Moment Estimators:
- Initial parameter guess for the prior distributions can be obtained using the moment method for the entire population.
#Note: merged_runs is a list of all runs including 0s
merged_runs = [item for sublist in initial_bayesian_df_runs['runs'] for item in sublist]

inital_guess_runs=get_moment_estimators(merged_runs)

print("The intial guess for Alpha and Beta via moment estimators would be:", inital_guess_runs)
initial_bayesian_df_runs = initial_bayesian_df_runs.assign(
    population_alpha=inital_guess_runs[0],
    population_beta=inital_guess_runs[1],
    # I dont have a fifth chain candidate for runs so I will use something random
    population_alpha0=random.uniform(1.5, 3),
    population_beta0=random.uniform(1.5, 3)
)
# Check the first few rows of the dataframe
#initial_bayesian_df_runs
from scipy.special import loggamma

def log_prior_run(alpha, beta, precision, lambda_hyper):
    
    theta_hyper = lambda_hyper*precision
    
    return theta_hyper * np.log(lambda_hyper) - loggamma(theta_hyper) + (theta_hyper - 1) * np.log(alpha + beta + 1) - lambda_hyper * (alpha + beta + 1) - np.log(alpha + beta)
def log_posterior_run(alpha, beta, data):
    
    precision = 5
    lambda_hyper = 0.5

    log_p = log_prior(alpha, beta, precision, lambda_hyper)
    
    for z_i in data:
        log_p +=  loggamma(alpha + beta) - loggamma(alpha) - loggamma(beta) + (alpha-1) * np.log(z_i) + (beta-1) * np.log(1-z_i)
            
    return log_p 
data = merged_runs
alpha_grid = np.linspace(0.1, 3, 100)
beta_grid = np.linspace(0.1, 3, 100)

log_posterior_grid = [[log_posterior_run(alpha, beta, data) for alpha in alpha_grid] for beta in beta_grid]

# To avoid underflow when exponentiating the log-density function, we first subtract the largest value,
# which means the largest value after exponentiation is always one.
posterior_grid = np.exp(log_posterior_grid - np.max(log_posterior_grid))

# Set up the plot
plt.figure(figsize=(12, 8))
contour = plt.contour(alpha_grid, beta_grid, posterior_grid, cmap='viridis')  # Using contour to show only the lines
plt.title("Contour plot for posterior of the whole population", fontsize=16)
plt.xlabel(r"$\alpha$", fontsize=14)
plt.ylabel(r"$\beta$", fontsize=14)
cbar = plt.colorbar(contour, label='Posterior Density')  # Adding a colorbar for clarity

# Add the point
point = [1.8466165867863291, 1.3678328084441114]
point_plot, = plt.plot(*point, 'ro', label='Prediction by the method of moment for the whole population')  # 'ro' means red color and 'o' marker

# Add the legend
plt.legend(handles=[point_plot], loc='upper right', fontsize=12)

# Display the plot
plt.grid(True, which='both', linestyle='--', linewidth=0.5)  # Adding a grid for better readability
plt.tight_layout()  # Adjusting the layout to fit all elements
plt.show()
def metropolis_algorithm_runs(data, initial_guess, number_of_samples):
    
    alphas = np.zeros((number_of_samples))
    betas = np.zeros((number_of_samples))
    
    alphas[0] = initial_guess[0] 
    betas[0] = initial_guess[1]

    for i in range(number_of_samples - 1):
        last_alpha = alphas[i]
        last_beta = betas[i]
        
        proposal_alpha =  np.exp(np.log(last_alpha) + stats.norm.rvs( 0.5, size=1))[0]
        proposal_beta =   np.exp(np.log(last_beta) + stats.norm.rvs(0.5, size=1))[0] 
        
        log_rho = log_posterior_run(proposal_alpha, proposal_beta, data) - log_posterior_run(last_alpha, last_beta, data)
        
        u = stats.uniform.rvs()
    
        if np.log(u) <= log_rho:
            alphas[i + 1] = proposal_alpha
            betas[i + 1] = proposal_beta
            
        else:
            alphas[i + 1] = last_alpha
            betas[i + 1] = last_beta
           
    return alphas, betas
data_list = initial_bayesian_df_runs.to_dict(orient='records')

results = []

for skateboarder in data_list:
    runs = skateboarder['runs']
    
    initial_guesses = [
        [skateboarder['alpha_run_Moment'], skateboarder['beta_run_Moment']],
        [skateboarder['alpha_run_Gradient'], skateboarder['beta_run_Gradient']],
        [skateboarder['alpha_run_Gauss'], skateboarder['beta_run_Gauss']],
        [skateboarder['population_alpha'], skateboarder['population_beta']],
        [skateboarder['population_alpha0'], skateboarder['population_beta0']]
    ]
    
    number_of_samples=15000
    # Run the Metropolis algorithm for each initial guess
    moment_chain = metropolis_algorithm_runs(runs, initial_guesses[0], number_of_samples)
    gradient_chain = metropolis_algorithm_runs(runs, initial_guesses[1], number_of_samples)
    gauss_chain = metropolis_algorithm_runs(runs, initial_guesses[2], number_of_samples)
    population_chain = metropolis_algorithm_runs(runs, initial_guesses[3], number_of_samples)
    population0_chain = metropolis_algorithm_runs(runs, initial_guesses[4], number_of_samples)
    
    # Store results in the specified format
    skateboarder['moment_chain'] = moment_chain
    skateboarder['gradient_chain'] = gradient_chain
    skateboarder['gauss_chain'] = gauss_chain
    skateboarder['population_chain'] = population_chain
    skateboarder['population0_chain'] = population0_chain
    
    results.append(skateboarder)

# Convert results back to DataFrame
initial_bayesian_df_runs = pd.DataFrame(results)
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
colors = sns.color_palette("deep", 5)

fig, axes = plt.subplots(nrows=len(initial_bayesian_df_runs), ncols=3, figsize=(20, 4 * len(initial_bayesian_df_runs)))

for idx, skateboarder in initial_bayesian_df_runs.iterrows():
    # Extract chains
    chains = [
        skateboarder['moment_chain'],
        skateboarder['gradient_chain'],
        skateboarder['gauss_chain'],
        skateboarder['population_chain'],
        skateboarder['population0_chain']
    ]
    
    # Extract and plot values
    for chain, color in zip(chains, colors):
        alpha_values, beta_values = chain
        
        # Plot cumulative average of alphas
        axes[idx, 0].plot(np.cumsum(alpha_values) / np.arange(1, len(alpha_values) + 1), color=color, linewidth=1.5)
        axes[idx, 0].set_title(f"{skateboarder['id']} - Alphas")
        axes[idx, 0].grid(True, which="both", linestyle='--', linewidth=0.5)
        
        # Plot cumulative average of betas
        axes[idx, 1].plot(np.cumsum(beta_values) / np.arange(1, len(beta_values) + 1), color=color, linewidth=1.5)
        axes[idx, 1].set_title(f"{skateboarder['id']} - Betas")
        axes[idx, 1].grid(True, which="both", linestyle='--', linewidth=0.5)
        
        # Scatter plot for alphas and betas in the third column
        axes[idx, 2].scatter(alpha_values, beta_values, color=color, s=10, alpha=0.5)
        axes[idx, 2].set_title(f"{skateboarder['id']} - Alphas vs Betas")
        axes[idx, 2].set_xlabel('Alphas')
        axes[idx, 2].set_ylabel('Betas')
        axes[idx, 2].grid(True, which="both", linestyle='--', linewidth=0.5)

# Adjust aesthetics for all plots
for ax_row in axes:
    for ax in ax_row:
        sns.despine(ax=ax)  # Remove top and right spines for a cleaner look
        ax.tick_params(axis="both", which="both", length=0)  # Remove tick marks

# Tight layout
plt.tight_layout()
plt.show()
initial_bayesian_df_runs['alpha_bayes_Y_mean'] = np.nan
initial_bayesian_df_runs['beta_bayes_Y_mean'] = np.nan
initial_bayesian_df_runs['alpha_bayes_Y_s^2'] = np.nan
initial_bayesian_df_runs['beta_bayes_Y_s^2'] = np.nan

chains = ['moment_chain', 'gradient_chain', 'gauss_chain', 'population_chain', 'population0_chain']

for idx, skateboarder in initial_bayesian_df_runs.iterrows():
    
    # Combine values from all chains
    all_alphas = np.concatenate([skateboarder[chain][0] for chain in chains])
    all_betas = np.concatenate([skateboarder[chain][1] for chain in chains])

    # Update the dataframe
    initial_bayesian_df_runs.at[idx, 'alpha_bayes_Y_mean'] = np.mean(all_alphas)
    initial_bayesian_df_runs.at[idx, 'beta_bayes_Y_mean'] = np.mean(all_betas)
    
    initial_bayesian_df_runs.at[idx, 'alpha_bayes_Y_s^2'] = np.var(all_alphas, ddof=1)
    initial_bayesian_df_runs.at[idx, 'beta_bayes_Y_s^2'] = np.var(all_betas, ddof=1)

initial_bayesian_df_runs[['id', "alpha_bayes_Y_mean", "beta_bayes_Y_mean", "alpha_bayes_Y_s^2", "beta_bayes_Y_s^2"]]
	id	alpha_bayes_Y_mean	beta_bayes_Y_mean	alpha_bayes_Y_s^2	beta_bayes_Y_s^2
0	Decenzo	3.373284	2.338858	1.960823	0.883608
1	Eaton	5.533491	2.316072	9.475568	1.501204
2	Foy	2.292370	2.815611	1.128809	1.766961
3	Gustavo	1.661208	1.252069	0.389875	0.214531
4	Hoban	3.321128	1.952833	1.668854	0.526214
5	Hoefler	1.544312	1.106628	0.435631	0.195936
6	Jordan	3.902773	1.502802	2.340452	0.290954
7	Majerus	1.645902	2.362721	0.729351	1.592044
8	Midler	1.021597	0.966702	0.366281	0.325968
9	Mota	2.846288	3.084403	1.716836	2.032120
10	Oliveira	3.489001	2.615413	2.560408	1.387751
11	O’neill	1.054346	1.308472	0.228759	0.385916
12	Papa	2.192996	2.156619	0.830945	0.807014
13	Ribeiro C	2.034782	1.838871	0.902972	0.715348
14	Santiago	2.149603	3.382969	0.974414	2.499454
15	Shirai	1.650113	1.154223	0.419162	0.181698
chains_list = ['moment', 'gradient', 'gauss', 'population', 'population0']

for chain_name in chains_list:
    for param in ['alpha', 'beta']:
        initial_bayesian_df_runs[f'{chain_name}_{param}_cumavg'] = pd.Series(dtype=object)

# Iterate through each row of the DataFrame
for idx, skateboarder in initial_bayesian_df_runs.iterrows():
    # Extract chains
    chains = [
        skateboarder['moment_chain'],
        skateboarder['gradient_chain'],
        skateboarder['gauss_chain'],
        skateboarder['population_chain'],
        skateboarder['population0_chain']
    ]
    
    # For each chain, compute cumulative averages and save to new columns
    for chain_name, chain in zip(chains_list, chains):
        alpha_values, beta_values = chain
        initial_bayesian_df_runs.at[idx, f'{chain_name}_alpha_cumavg'] = cumulative_avg(alpha_values)
        initial_bayesian_df_runs.at[idx, f'{chain_name}_beta_cumavg'] = cumulative_avg(beta_values)
    ## (d) 
Use your Bayesian model for $[X_i, Y_i]^T$ to simulate 5000 LCQs by drawing samples from the appropriate posterior predictive distributions. 

- What is the mode of your outcomes $W_1, \ldots, W_{5000}$? 

- How many of the real winners are predicted? 

- What is the estimated probability of the real winners based on your samples? 

- And by the mode?
bayesian_trick_data = pd.DataFrame()

# Extract the 'id' column and add it to the new dataframe
bayesian_trick_data['id'] = initial_bayesian_df_tricks['id']

# Extract the columns ending in '_cumavg'
cumavg_cols = [col for col in initial_bayesian_df_tricks.columns if col.endswith('_cumavg')]

# Group columns by chain prefix
chains_list = ['moment', 'gradient', 'gauss', 'population', 'population0']
cumavg_grouped = {chain: [col for col in cumavg_cols if chain in col] for chain in chains_list}

# For each row in initial_bayesian_df_tricks, extract the last 1000 elements from the cumavg columns
# and use them to construct the 5000 3-element vectors.
trick_vectors_list = []
for _, row in initial_bayesian_df_tricks.iterrows():
    vectors = []
    for chain in chains_list:
        # For each of the 1000 elements, construct the 3-element vector
        for j in range(-1000, 0):  # Indexing from the end of the list to get the last 1000 elements
            vector = [row[cumavg_grouped[chain][0]][j], row[cumavg_grouped[chain][1]][j], row[cumavg_grouped[chain][2]][j]]
            vectors.append(vector)
    trick_vectors_list.append(vectors)

# Add the trick_vectors to the new dataframe
bayesian_trick_data['trick_vectors'] = trick_vectors_list

bayesian_trick_data
bayesian_run_data = pd.DataFrame()

# Extract the 'id' column and add it to the new dataframe
bayesian_run_data['id'] = initial_bayesian_df_runs['id']

# Extract the columns ending in '_cumavg'
cumavg_cols_runs = [col for col in initial_bayesian_df_runs.columns if col.endswith('_cumavg')]

# Group columns by chain prefix
chains_list_runs = ['moment', 'gradient', 'gauss', 'population', 'population0']
cumavg_grouped_runs = {chain: [col for col in cumavg_cols_runs if chain in col] for chain in chains_list_runs}

# For each row in initial_bayesian_df_runs, extract the last 1000 elements from the cumavg columns
# and use them to construct the 5000 2-element vectors.
run_vectors_list = []
for _, row in initial_bayesian_df_runs.iterrows():
    vectors = []
    for chain in chains_list_runs:
        # For each of the 1000 elements, construct the 2-element vector
        for j in range(-1000, 0):  # Indexing from the end of the list to get the last 1000 elements
            vector = [row[cumavg_grouped_runs[chain][0]][j], row[cumavg_grouped_runs[chain][1]][j]]
            vectors.append(vector)
    run_vectors_list.append(vectors)

# Add the run_vectors to the new dataframe
bayesian_run_data['run_vectors'] = run_vectors_list

bayesian_run_data
# Here I just want to check if it works for bayesian_run_data!
print("The length is:", len(bayesian_run_data.iloc[0]['run_vectors']))

# Checking the first 5 as an example.
for vector in bayesian_run_data.iloc[0]['run_vectors'][:5]:
    print(vector)

# Check that all vectors have length 2.
for idx, row in bayesian_run_data.iterrows():
    if not all(len(vector) == 2 for vector in row['run_vectors']):
        print(f"Skateboarder with id {row['id']} has a vector not of length 2!")
def simulate_lcq_bayes(chosen_trick_data_df, chosen_run_data_df, lcq_idx):
    new_lcq_df = pd.DataFrame(columns=["id", "run 1", "run 2", "trick 1", "trick 2", "trick 3", "trick 4"])
    
    for index, (trick_row, run_row) in enumerate(zip(chosen_trick_data_df.iterrows(), chosen_run_data_df.iterrows())):
        _, trick_row = trick_row
        _, run_row = run_row

        alpha_trick, beta_trick, theta = trick_row['trick_vectors'][lcq_idx]
        alpha_run, beta_run = run_row['run_vectors'][lcq_idx]

        new_lcq_df.at[index, "id"] = trick_row['id']
        
        for _ in range(4):
            if successful_trick(theta):
                trick = trick_score(alpha_trick, beta_trick)
            else:
                trick = 0
            new_lcq_df.at[index, f"trick {_+1}"] = round(trick,1)
        
        for _ in range(2):
            run = run_score(alpha_run, beta_run)
            new_lcq_df.at[index, f"run {_+1}"] = round(run,1)
        
        total_score_value = total_score(new_lcq_df.loc[index, ['trick 1', 'trick 2', 'trick 3', 'trick 4']], new_lcq_df.loc[index, ['run 1', 'run 2']]) 
        new_lcq_df.at[index, "total score"] = round(total_score_value,1)
    
    return new_lcq_df
top_four_rankings = []
point_counts = {}

# Iterate over all LCQs (0 to 4999)
for lcq_idx in range(5000):
    lcq_results = simulate_lcq_bayes(bayesian_trick_data, bayesian_run_data, lcq_idx)
    top_four_df = find_top_four_skateboarders(lcq_results)

    # Extract IDs of the top four skateboarders
    top_four_ids = top_four_df['id'].tolist()[:4]
    top_four_rankings.append(top_four_ids)

    # Loop through the top 4 skateboarders and award them a point
    for i in range(4):
        skateboarder_id = top_four_df.iloc[i]['id']
        point_counts[skateboarder_id] = point_counts.get(skateboarder_id, 0) + 1

print("\n\nFor Bayesian Model:")
find_mode_and_occurences_of_specific(top_four_rankings, ["Gustavo", "Decenzo", "Eaton", "Hoban"])

# Convert the point_counts dictionary to a DataFrame and rank the skateboarders
df_ranked_bayes = pd.DataFrame(list(point_counts.items()), columns=['id', 'points'])
df_ranked_bayes = df_ranked_bayes.sort_values(by='points', ascending=False)
df_ranked_bayes['rank'] = df_ranked_bayes['points'].rank(ascending=False)
